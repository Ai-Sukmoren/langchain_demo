{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken \n",
    "from typing import Union, List\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.tools import Tool\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain.agents.output_parsers import ReActSingleInputOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from tools.callbacks import AgentCallbackHandler\n",
    "\n",
    "#load variable from .env\n",
    "load_dotenv()  \n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = PyPDFLoader(file_path=pdf_path)\n",
    "# documents = loader.load()\n",
    "# text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "# chunked_docs = text_splitter.split_documents(documents=documents)\n",
    "# # Count the chunks\n",
    "# num_chunks = len(chunked_docs)\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "# vectorstores = FAISS.from_documents(chunked_docs, embeddings)\n",
    "# vectorstores.save_local(\"damac_database\")\n",
    "# damac_vectorstore = FAISS.load_local(\"damac_database\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the gratuity that the lessee is obligated to pay to the lessor under the lease agreement?\n"
     ]
    }
   ],
   "source": [
    "#assign variables\n",
    "Question = \"\"\"\n",
    "        Subject to Lease GRATUITY what is the about that lessee must pay to the Lessor\n",
    "        \"\"\"\n",
    "input_language = 'TH'\n",
    "output_language = 'EN'\n",
    "\n",
    "#promptTemplate\n",
    "summary_templates = \"\"\"\n",
    "   You are a translation specialist with 35 years of experience in translating from {input_language} to {output_language}. Given the question {Question}, \n",
    "   your task is to translate it into {output_language}. If the question is already in {output_language}, check for any grammatical mistakes and correct them if necessary.\n",
    "    Caution:\n",
    "    Provide only the answer, without any explanations.\n",
    "    \"\"\"\n",
    "\n",
    "summary_prompts_template = PromptTemplate(input_variables=['Question','input_language','output_language'], template = summary_templates)\n",
    "\n",
    "llm = ChatOpenAI(temperature= 0, model_name=\"gpt-4\",api_key=openai_api_key)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=summary_prompts_template) \n",
    "\n",
    "Question = chain.run(Question=Question,input_language=input_language,output_language=output_language)\n",
    "\n",
    "print(Question)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! stop is not default parameter.\n",
      "                    stop was transferred to model_kwargs.\n",
      "                    Please confirm that stop is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The gratuity that the lessee is obligated to pay to the lessor is THB 260,000,000 (two hundred and sixty million Thai Baht).\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_ans_damac(Question: str) -> str:\n",
    "    \"\"\"Returns the answer based on the question related to Agreement\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    damac_vectorstore = FAISS.load_local(\"damac_database\", embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), chain_type=\"stuff\", retriever=damac_vectorstore.as_retriever(),\n",
    "    )\n",
    "    result = qa.run(Question)\n",
    "    return result\n",
    "\n",
    "@tool\n",
    "def get_ans_elon(Question: str) -> str:\n",
    "    \"\"\"Returns the answer based on the question related to ELONMUSK\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    elonmusk_vectorstore = FAISS.load_local(\"elonmusk_database\", embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(), chain_type=\"stuff\", retriever=elonmusk_vectorstore.as_retriever(),\n",
    "    )\n",
    "    result = qa.run(Question)\n",
    "    return result\n",
    "\n",
    "def find_tool_by_name(tools: List[Tool], tool_name: str) -> Tool:\n",
    "    for tool in tools:\n",
    "        if tool.name == tool_name:\n",
    "            return tool\n",
    "    raise ValueError(f\"Tool wtih name {tool_name} not found\")\n",
    "\n",
    "\n",
    "tools = [get_ans_damac, get_ans_elon]\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template).partial(\n",
    "    tools=render_text_description(tools),\n",
    "    tool_names=\", \".join([t.name for t in tools]),\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    stop=[\"\\nObservation\"], \n",
    "    model = \"gpt-4\",\n",
    "    api_key=openai_api_key,\n",
    ")\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | ReActSingleInputOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "agent_step: Union[AgentAction, AgentFinish] = agent.invoke(\n",
    "            {\"input\": Question}\n",
    "        )\n",
    "\n",
    "if isinstance(agent_step, AgentAction):\n",
    "            tool_name = agent_step.tool\n",
    "            tool_to_use = find_tool_by_name(tools, tool_name)\n",
    "            tool_input = agent_step.tool_input\n",
    "\n",
    "result = tool_to_use.func((str(tool_input)))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ค่าของความเป็นของขวัญที่ผู้เช่ามีภาระหน้าที่ต้องจ่ายให้กับผู้ให้เช่าคือ 260,000,000 บาท (สองร้อยหกสิบล้านบาทไทย)\n"
     ]
    }
   ],
   "source": [
    "#assign varible\n",
    "output = result\n",
    "\n",
    "#promptTemplate\n",
    "summary_templates = \"\"\"\n",
    "   You are a translation specialist with 35 years of experience in translating from {input_language} to {output_language}. \n",
    "   Given the information {output}, your task is to translate it into {input_language}.\n",
    "    Caution:\n",
    "    Only provide the answer with no explanation.\n",
    "    \"\"\"\n",
    "\n",
    "summary_prompts_template = PromptTemplate(input_variables=['output','input_language','output_language'], template = summary_templates)\n",
    "\n",
    "llm = ChatOpenAI(temperature= 1, model_name=\"gpt-4\",api_key=openai_api_key)\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=summary_prompts_template) \n",
    "\n",
    "Output = chain.run(output=output,input_language=input_language,output_language=output_language)\n",
    "\n",
    "print(Output)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
