{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "19e92812-b9a1-4238-9e5a-b8a68ce4dfd0\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "# Print the API key (for demonstration purposes)\n",
    "print(openai_api_key)\n",
    "print(pinecone_api_key)\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(api_key=pinecone_api_key, environment='gcp-starter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: C:\\Users\\(Ai)AiSukmoren\\Desktop\\langchain_demo\\doc\\Elon_mush.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\(Ai)AiSukmoren\\Desktop\\langchain_demo\\doc\\Elon_mush.txt\"  # Use raw string for file path\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    print(f\"File found: {file_path}\")\n",
    "\n",
    "# Rest of your code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello vector\n",
      "1\n",
      "{'query': '\\n    who is elon musk?\\n    ', 'result': \" I don't know.\", 'source_documents': [Document(page_content='For queries, developers ask the machine learning model for a representation (embedding) of just that query. Then the embedding can be passed to the vector database, and it can return similar embeddings â€” which have already been run through the model. Those embeddings can then be mapped back to their original content: whether that is a URL for a page, a link to an image, or product SKUs.', metadata={'source': 'hello.txt'}), Document(page_content='For queries, developers ask the machine learning model for a representation (embedding) of just that query. Then the embedding can be passed to the vector database, and it can return similar embeddings â€” which have already been run through the model. Those embeddings can then be mapped back to their original content: whether that is a URL for a page, a link to an image, or product SKUs.', metadata={'source': 'hello.txt'}), Document(page_content='When one visits a shoe store, a salesperson may suggest shoes that are similar to the pair one prefers. Likewise, when shopping in an ecommerce store, the store may suggest similar items under a header like \"Customers also bought...\" Vector databases enable machine learning models to identify similar objects, just as the salesperson can find comparable shoes and the ecommerce store can suggest related products. (In fact, the ecommerce store may use such a machine learning model for doing so.)', metadata={'source': 'hello.txt'}), Document(page_content='This saves a tremendous amount of processing time. It makes building user-facing applications around semantic search, classification, and anomaly detection possible, because results come back within tens of milliseconds, without waiting for the model to crunch through the whole data set.\\n\\nFor queries, developers ask the machine learning model for a representation (embedding) of just that query. Then the embedding can be passed to the vector database, and it can return similar embeddings â€” which have already been run through the model. Those embeddings can then be mapped back to their original content: whether that is a URL for a page, a link to an image, or product SKUs.\\n\\nTo summarize: Vector databases work at scale, work quickly, and are more cost-effective than querying machine learning models without them.', metadata={'source': 'C:\\\\Users\\\\(Ai)AiSukmoren\\\\Desktop\\\\intro-to-vector-db\\\\hello.txt'})]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"hello vector\")\n",
    "    \n",
    "    # Use a raw string for the file path\n",
    "    loader = TextLoader(r\"C:\\Users\\(Ai)AiSukmoren\\Desktop\\langchain_demo\\doc\\Elon_mush.txt\")\n",
    "\n",
    "    document = loader.load()\n",
    "    \n",
    "    text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=25)\n",
    "    texts = text_splitter.split_documents(document)\n",
    "    #chunk unit\n",
    "    print(len(texts))\n",
    "    \n",
    "    # enable embedding of thr text\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    docsearch = Pinecone.from_documents(texts,embeddings,index_name=\"medium-blogs-embedding-index\")\n",
    "    \n",
    "    \n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(), chain_type=\"stuff\", retriever=docsearch.as_retriever(), return_source_documents=True,\n",
    "    )\n",
    "    \n",
    "    query = \"\"\"\n",
    "    who is elon musk?\n",
    "    \"\"\"\n",
    "    result = qa({\"query\":query})\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
